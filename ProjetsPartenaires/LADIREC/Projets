Qu’est-ce que le corpus recits faim? 
Le corpus recits faim est un jeu de données de plus de 40 milles articles de presse concernant la sécurité alimentaire à Montréal, publiés entre janvier 2005 et mai 2020 et provenant de ving-trois sources médiatiques en ligne. Les articles sont bilingues, avec 80% des documents écrits en français et 20% en anglais. 

La question de recherche 
La question de recherche de départ selon laquelle le corpus fut construit peut être formulée comme suit: «Lorsque les médias traitent de l’alimentation à Montréal entre 2005 et 2020, à quelle(s) partie(s) du circuit de l’alimentation humaine renvoient-ils?» Les «partie(s) du circuit de l’alimentation humaine» désignent la production, l’entreposage et la distribution, la vente au détail et la consommation, ainsi que la gestion des déchets. 

La constitution du corpus 
La constitution du corpus fut effectuée en quatre étapes: 
L’identification des articles potentiellement pertinents et le recueillement de leurs adresses URL; 
Le moissonnage, c’est-à-dire l’obtention du contenu des articles (ex. titre, sous-titres, corps du texte, etc.) à partir de leurs adresses URL, ainsi que l’organisation de ces données sous forme de tableaux; 
Le prétraitement des données textuelles sous forme de base de données pour les préparer à l’analyse par traitement informatique; 
La classification pour ne garder que les documents pertinents. 
Chaque étape est expliquée plus en détail ci-dessous. 

Identification des articles 
Sur le moteur de recherche Google, des requêtes par mots-clés ou chaînes de caractères furent utilisées pour identifier les articles potentiellement pertinents à la question de recherche. L’extension logicielle gInfinity permettait d’afficher tous les résultats sur une seule page; les URL de ces résultats ont pu être compilés et extraits à l’aide de l’outil Google SERPs Extractor. 
Pour les médias de portée locale, les requêtes ont été faites avec les termes alimentation, bouffe, et food seulement. Pour les médias de portée nationale, ces requêtes ont combiné le nom de la ville, montreal, à l’un ou l’autre des termes alimentation, bouffe, ou food. 

Moissonnage 
Pour extraire le contenu des articles selon des catégories distinctes (titre, sous-titres, corps du texte, etc.), il fallait d’abord identifier les balises HTML qui structurent ces textes. Pour ce faire, un échantillon de pages présentant des variations fut étudié. 
Ces balises ont servi à construire des patrons de requête (sitemaps) dans l’extension logicielle Web Scraper du navigateur Chrome. Les adresses URL des articles identifiés furent ensuite manipulées, puis insérées dans les patrons de requête. Ces-derniers permettaient ainsi d’extraire des textes l’information correspondant aux balises HTML. 
Un exemple de patron de requête. La partie surlignée correspond à l’endroit où les liens URL des articles sont insérés. Le reste indique les balises à moissonner (ex. Article header, article section, article footer). 
Finalement, les patrons de requête furent reversés dans Web Scraper, qui prenait en charge la procédure de moissonnage. Le logiciel générait des tableurs affichant les résultats du moissonnage (i.e. l’information correspondant à chaque balise pour chaque article). Ces tableurs furent copiés et versés dans le logiciel de manipulation de données OpenRefine. 

Procédures de prétraitement 
Plusieurs mesures furent prises pour nettoyer les données textuelles moissonnées, de sorte d’obtenir un texte propre où toute information (ex. année de publication, nom de l’auteur, titre, etc.) puisse être traitée comme des éléments distincts dans l’ensemble de données final. Pour chacune des sources médiatiques, les données extraites furent versées dans un tableur, structurées et nettoyées. Les résultats des requêtes concurrentes ont été comparées, les exceptions, identifiées, les valeurs, selon le besoin, divisées ou concaténées. Ceci est une étape importante pour préparer le corpus à l’analyse, car les résultats dépendent fortement de la propreté des données. 

Classification 
L’opération de classification consiste à sélectionner d’abord manuellement les documents pertinents du corpus selon des critères définies dans un guide de classification développé par le LADIREC. Selon ce guide, dans le corpus final, tous les documents font référence à au moins une des catégories suivantes: 

Production 
Ex.: agriculture, serriculture, apiculture, élevage animal destiné à la consommation, production d'alcools et de boissons non alcoolisées, production de plats et d'autres produits destinés à l'alimentation, empaquetage 

Entreposage et distribution 
Ex.: transport des aliments entre leurs lieux de production et leur point de vente ou de don, chaîne d’approvisionnement 

Vente au détail et consommation 
Ex : accès aux marchés et aux banques de dons, marketing, prix de vente, restauration, bars, cafés 

Gestion des déchets 
Ex. : compost, enfouissement, revalorisation 

Communications 
Ex. : livres, conférences, comme sujet de débat politique 
Les documents ne sont pas classifiés selon leur degré de pertinence: la présence d’au moins une expression ou d’un mot pertinent à la question de recherche suffit pour qu’un document soit considéré comme pertinent. Puis, nous avons formé un algorithme d’apprentissage automatique pour ne garder que les documents pertinents. Il est à noter que les documents écartés à cette étape ne sont pas détruits, mais pourraient servir à des analyses subséquentes. 


Que peut-on faire avec le corpus? 
Nos projets 
Banque alimentaire (Lisa Teichmann) 
Nous avons mené une étude de cas sur la représentation des banques alimentaires dans notre corpus. Nous avons constaté que les banques alimentaires sont principalement mentionnées dans les médias locaux qui couvrent certains quartiers que nous avons pu visualiser sur une carte. Ce projet a souligné l'importance des petites sources de nouvelles dans la diffusion de l'information sur l'accès aux banques alimentaires. Nous avons utilisé nos outils de regroupement d'articles par sujet (dans ce cas les banques alimentaires) et de géomapping des articles mentionnant des arrondissements et des quartiers en utilisant les données disponibles par Données Québec. 
Sur demande, les outils utilisés pour les banques alimentaires peuvent également être appliqués à d'autres sujets tels que les organisations alimentaires, les marchés alimentaires, les événements, etc.  
La traduction automatique en chaine (ELisabeth Doyon) 
Le projet de traduction en chaine a plus d’un objectif. Tout d’abord, il nous est utile de traduire automatiquement le contenu du corpus médiatique sur l’alimentation pour certaines analyses puisqu’il contient deux langues, en anglais et en français. Aussi, la mise en série de traduction d’une langue à une autre, puis d’un retour à la langue originale transforme la traduction automatique en une fonction de reformulation de phrase ; la production de paraphrase. Cette fonction peut servir à augmenter le nombre d’exemple pour l’entrainement d’un classificateur automatique qui utilise de l’apprentissage-machine supervisée. Il s’agit d’une manière de multiplier la valeur d’un ensemble de données qui aurait été manuellement catégorisé selon n’importe quelle grille d’analyse. 
Café-café (Amelie Ducharme) 
Le projet « café-café » avait pour but de désambiguïser deux termes, « café » le lieu et « café » la chose (breuvage, grain, industrie…), afin d’aider à repérer automatiquement les mentions d’emplacements géographiques dans les textes analysés par le Laboratoire d’analyse des discours et récits collectifs (LADIREC). En utilisant un corpus médiatique sur l’alimentation composé de 46 513 articles récoltés par le LADIREC, nous avons d’abord réduit le nombre de textes en conservant seulement ceux dans lesquels nous avons trouvé, grâce à une expression régulière, certaines variations du mot « café » (avec ou sans accent/majuscule/marque du pluriel). Ensuite, nous n’avons retenu que les articles de langue française, puisque l’ambiguïté du mot « café » est inexistante en anglais (où « coffee house » et « cafe » se distinguent plus clairement du simple « coffee »). Ensuite, grâce à des procédés de vectorisation (word-to-vec), nous avons pu déterminer quels termes du sous-corpus étaient les plus similaires à chaque variation de « café » et, donc, quelles tendances se dégagent de l’emploi de chaque variation (catégorie grammaticale, accord, champ lexical, etc.). Nous avons également usé de fenêtres contextuelles (keyword-in-context) pour analyser les segments qui précèdent et suivent chaque occurrence de « café », encore dans le but de dégager les tendances quant à l’emploi de chaque variation et, ultimement, de formuler des hypothèses de stratégies qui rendraient l’ordinateur capable de distinguer les deux sens du mot « café » lors d’une analyse automatisée. 
Indice de centralité sur l’insécurité alimentaire (Elisabeth Doyon) 
Le projet de création de l’indice de centralité sur l’insécurité alimentaire a pour objectif de réduire les dimensions inhérentes au texte et aux discours médiatiques (complexe et dont la lecture prend du temps) à un indice qui pourrait servir d’alerte ou d’indicateur. Lorsque les médias parlent d’alimentation, ils parlent de sujets en fait différents, parfois de restaurant, de sorties et de gastronomie, parfois aussi de banque alimentaire, de trucs pour faire des économies ou éviter le gaspillage. Ces différents discours – et surtout les termes utilisés pour parler de l’alimentation – peuvent être associés à des scores qui placent sur l’axe singulier de la sécurité alimentaire chaque article de presse. À partir de cette réduction, il est alors possible de naviguer, comparer et cibler le matériel médiatique selon une autre dimension, celle du territoire ou des périodes temporelles. La création de l’indice de centralité inclut le développement d’outils de traitement de texte, mais aussi des étapes de consultation pour établir les scores associés aux différents enjeux de la sécurité alimentaire. 
Géolocalisation (Lisa Teichmann) 
Les outils et stratégies de géocartographie ont été développés pour répondre à des questions telles que : quels sont les quartiers de Montréal qui sont sous-représentés lorsque l'on parle d'insécurité alimentaire dans les médias d'information? Quels sont ceux qui sont mis en évidence ? Quelle est le rôle des médias d'information pour fournir aux résidents des informations sur comment et où accéder aux services alimentaires ? Nous avons conçu un modèle de géodonnées qui nous a permis de collecter et d'utiliser les différents jeux de données mis à notre disposition par diverses sources telles que Données Québec, les paroisses et les partenaires. Nous avons développé plusieurs stratégies pour extraire des textes du corpus les mentions de lieux, telles que les adresses, les arrondissements et les quartiers. Notre modèle nous permet de situer les adresses apparaissant dans les articles dans les limites des arrondissements ou des quartiers, ce qui nous donne la flexibilité de visualiser les dimensions géographiques dans le corpus. Nous élargissons continuellement notre ensemble de données en collectant des données auprès de sources supplémentaires. Nous aussi sommes en contact avec le GIC (Geographic Information Centre) de McGill pour une collaboration potentielle. 
Traduction automatique/sommaire (Elisabeth Doyon) 
Le script de synthèse prend un corpus de texte structuré en entrée et résume la ou les colonnes choisis en début du script et dépose les synthèses dans une table en sortie. Le script de traduction prend un corpus de texte structuré en entrée, établit une chaine de traduction d'une langue à une autre pour produire des variations syntaxiques des textes envoyés tout en conservant la sémantique puis dépose les paraphrases dans une table en sortie. Il utilise le cadre de travail de Hugging face ou sont déposés des modèles de langues tiré de différentes stratégies. Ce script utilise les modèles de type BERT et applique les contraintes de ce type de modèle (longueur maximale, type de lemmatisation etc.). Il est possible de modifier le modèle utilisé à même le code. 
La synthèse automatique (Elisabeth Doyon) 
La synthèse automatique nous sert pour reformuler les articles d'un corpus de textes. Elle est en soi une forme de paraphrase qui raccourcit le texte en utilisant un modèle de langue de type BERT. Selon le type d'entente et de licence obtenu avec les propriétaires, des textes permettent ou non la diffusion des textes originaux ayant servi aux analyses. La production de synthèse automatique permet de partager des résumés des articles analysés afin de rendre plus transparentes nos analyses et donner accès au contexte des résultats. Cette méthode de transformation du texte est imparfaite et est particulièrement déficiente pour certains types d'articles (les petites annonces par exemple). Elle rend tout de même possible le retour au corpus tout en respectant les ententes légales du partage des documents. 
Subdivision automatique du jeu de donnée (Elisabeth Doyon) 
Ce projet vise à produire un outil pour gérer la segmentation des grands corpus de texte pour les lourdes computations. Dans le traitement de nos données textuelles, il arrive que le temps de computation soit énorme. On parle ici d'une semaine ou plus de traitement pour une seule fonction. C'est le cas notamment lorsqu'un modèle de langue est utilisé, surchargeant la mémoire et les capacités de traitement. Il est alors préférable de perdre un peu en termes d'optimisation pour récupérer au fur et à mesure les résultats d'un traitement pour réduire le risque d'avoir à reprendre le travail depuis le début. Ce 'batching' segmentera le corpus de texte selon un ensemble de paramètres choisis par l'utilisateur qui décidera la fréquence pour les rejoindre pour l’export. 

Projets à venir 
Nous travaillons sur l’addition d’un jeu de données sur les paroisses de Montréal à nos géodonnées, ce qui nous permettra de cartographier non seulement les quartiers, mais aussi les limites des paroisses. Ceci est particulièrement important pour des services alimentaires tels que les banques alimentaires, qui opèrent dans les communautés de façon très régionalisée. 

De plus, à la demande de nos partenaires, nous prévoyons explorer l’analyse de sentiments pour des arrondissements et des quartiers spécifiques, ainsi que des outils pour décrire le paysage médiatique du discours sur la sécurité alimentaire. 

 
Include: 

Links to projects, papers etc. 

 

Nous sommes ravis d’annoncer que nos travaux sur la traduction automatique seront présentés à la conférence CATS/ACT au York University en mai/juin 2023 par les membres du LADIREC Karolina Roman, Elisabeth Doyon, et Lisa Teichmann. 

 

Un article de Pascal Brissette et Julien Vallière sur les « food trucks » figurera dans le prochain numéro de la revue À Bâbord, à paraître en ___________ 

 

Link to corpus (Format due to license restriction (TFIDF and 25%)) 

 

Les scripts pour le processus de classification, le projet café/café, et le résumé/traduction automatique sont maintenant disponibles sur notre dépôt Github! 

 

Lien URL: https://github.com/LADIREC 

 

Notre corpus sera publié au début de l’année prochaine sur Dataverse. 

 

Ressources: if you need help, you can reach out to us 

Contact info (who do we put?) 

Ateliers (LINK!) 

Guides de moissonnage (Webscraper),  de constitution de corpus et de nettoyage (Word)? 

 

 

 

 
 

Infolettre du PDS 

Date limite: Lundi 7 novembre 2022 

https://www.mcgill.ca/centre-montreal/fr/ressources/infolettre 

 

 

Sujet de la nouvelle  

Le corpus du projet «Les récits de la faim à Montréal» est maintenant disponible! 

 

Courte description [quoi, quand, qui, pourquoi et comment]  

Dans le cadre du Pôle d’analyse de données sociales, notre Laboratoire d’analyse des discours et des récits collectifs (LADIREC) a le plaisir d’annoncer la publication de notre corpus d’analyse sur les récits de la faim. Ce corpus, composé de plus de 45 000 articles de presse québécois (33 213 en français et 13 300 en anglais) publiés entre 2005 et 2020, est maintenant librement accessible sur la plateforme Dataverse.  

 

Nous invitons la communauté de recherche à explorer et à utiliser cette ressource à des fins diverses, telles que l’analyse de discours, la fouille de texte, et la géocartographie. D’ailleurs, le LADIREC a déjà développé certains outils pour travailler avec ce type de donnée à travers nos ateliers qui étaient offerts durant le printemps de l’année 2022. 

 

We already developed tools to work with this type of data through our ateliers, given during Spring 2022 

Banque alimentaire 

Café-café 

Sécurité alimentaire 

 

Dates importantes à retenir  

Date de publication 

Date de la présentation (à déterminer) 

Adresse du site web à consulter   

URL vers une page web avec plus d’information (bouton type « en savoir plus »?) 

URL vers le jeu de données (DOI de Dataverse Dataset) 
